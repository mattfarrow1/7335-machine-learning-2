# -*- coding: utf-8 -*-
"""ML2_HW4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1skR1AWpuXmCdhYBGIV-Z94Jd3Uuj62J-

## ML2 HW4
### Megan Ball, Matt Farrow, Blake Freeman

Below you see a tutorial from Keras on using transfer learning.  
 They train their models on have the digits and predict the second half.   
 Your homework is to train on all digits and make your own handwritten data set of 5 characters (ie A, B, C, D, E)  
 and transfer your minist trained model over to them.  Enjoy!
"""

#import packages
import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model

#set random seed for reproducibility
from numpy.random import seed
seed(123)
tf.random.set_seed(45)

"""## Importing Data

Below text and model creation adapted from example script provided in office hours.
"""

# load in the dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(x_train.shape)

print(y_train.shape)

# Look at some random numbers
X_shuffle = shuffle(x_train.copy(), random_state=42)

print('Look at some numbers\n')
plt.figure(figsize = (12,10))
row, colums = 4, 4
for i in range(16):
    plt.subplot(colums, row, i+1)
    plt.imshow(X_shuffle[i].reshape(28,28),interpolation='nearest', cmap='Greys')
plt.show()

"""## Data Preprocessing

We need to do the following to preprocess the data:

1. Divide the values by the max value (255)
2. Add a color channel (require by the convolutional layer)

By default Keras assumes images are formatted as (number of examples, x-dim, y-dim, number of colors)

An RGB image has 3 color channels, greyscale as 1 color channel
"""

x_train.max()

# divide by the color channel
x_train = x_train / 255.
x_test = x_test / 255.

# add the color channel
x_train = x_train.reshape(x_train.shape + (1,))
x_test = x_test.reshape(x_test.shape + (1,))

x_train.shape

"""## Setup the Model

This is a very basic convolutional network.
There are essentially two sections
1. convolutional feature extraction layers
2. dense (fully-connected) classifier layers
"""

num_classes=10
filters=32
kernel_size=3
pool_size=2
dropout=0.2
input_shape = (28,28,1)

model = Sequential([
      # convolutional feature extraction
      # ConvNet 1

        # convoultional part
      keras.layers.Conv2D(filters, kernel_size, padding = 'valid',
              activation='relu',
              input_shape=input_shape),
        # pooling part
      keras.layers.MaxPooling2D(pool_size=pool_size),

      # ConvNet 2
      
        # convoultional part
      keras.layers.Conv2D(filters, kernel_size,
              padding = 'valid',
              activation='relu'),
        # pooling part
      keras.layers.MaxPooling2D(pool_size=pool_size),

      # classification 
      # will retrain from here
      keras.layers.Flatten(name='flatten'),

      # fully conencted layer 1
      keras.layers.Dropout(dropout),
      keras.layers.Dense(128, activation='relu'),
      
      # fully connected layer 2
      keras.layers.Dropout(dropout, name='penult'),
      keras.layers.Dense(num_classes, activation='softmax', name='last')
  ])

# print summary of model as check
model.summary()

# print image of model as check
keras.utils.plot_model(model, show_shapes=True, dpi=48)

es = keras.callbacks.EarlyStopping(min_delta=0.001, patience=2)

"""## Loss Functions

When y labels are sequential use `sparse_categorical_crossentropy`.  
When y labels are onehot encoded, use `categorical_crossentropy`.
"""

model.compile(loss='sparse_categorical_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    validation_split=0.2,
                    batch_size=32,
                    epochs=1000,
                    callbacks=[es])

def plot_training_curves(history, title=None):
    ''' Plot the training curves for loss and accuracy given a model history
    '''
    # find the minimum loss epoch
    minimum = np.min(history.history['val_loss'])
    min_loc = np.where(minimum == history.history['val_loss'])[0]
    # get the vline y-min and y-max
    loss_min, loss_max = (min(history.history['val_loss'] + history.history['loss']),
                          max(history.history['val_loss'] + history.history['loss']))
    acc_min, acc_max = (min(history.history['val_accuracy'] + history.history['accuracy']),
                        max(history.history['val_accuracy'] + history.history['accuracy']))
    # create figure
    fig, ax = plt.subplots(ncols=2, figsize = (15,7))
    fig.suptitle(title)
    index = np.arange(1, len(history.history['accuracy']) + 1)
    # plot the loss and validation loss
    ax[0].plot(index, history.history['loss'], label = 'loss')
    ax[0].plot(index, history.history['val_loss'], label = 'val_loss')
    ax[0].vlines(min_loc + 1, loss_min, loss_max, label = 'min_loss_location')
    ax[0].set_title('Loss')
    ax[0].set_ylabel('Loss')
    ax[0].set_xlabel('Epochs')
    ax[0].legend()
    # plot the accuracy and validation accuracy
    ax[1].plot(index, history.history['accuracy'], label = 'accuracy')
    ax[1].plot(index, history.history['val_accuracy'], label = 'val_accuracy')
    ax[1].vlines(min_loc + 1, acc_min, acc_max, label = 'min_loss_location')
    ax[1].set_title('Accuracy')
    ax[1].set_ylabel('Accuracy')
    ax[1].set_xlabel('Epochs')
    ax[1].legend()
    plt.show()

plot_training_curves(history)

# predict using the model
preds = model.predict(x_test)

x_test.shape

preds.shape

preds[0]

# classify the test set

# predict using the model
preds = model.predict(x_test)
# argmax along rows to get classification
preds = np.argmax(preds, axis=1).astype("uint8")

accuracy_score(y_test, preds)

preds.shape

preds

"""# Using Our Own Dataset

> From this point on, we adapt Stuart's code using our own handwritten letters. We make a training set of 55 letters and a test/validation set of 5 letters (one of each letter).

## Load Train Set
"""

!pip install pillow
import numpy as np
from PIL import Image

import matplotlib.pyplot as plt
import glob

# import handwritten letters

images = glob.glob("drive/My Drive/train_letters/*.jpg")
images

# pull labels for y target labels

image_labels = []
for i in images:
  image_letter = i[29]
  print(image_letter)
  image_labels.append(image_letter)

len(image_labels)

#convert to array
train_labels = np.array(image_labels)

im = Image.open(images[0])
plt.imshow(im);

size = (28,28)
im = im.resize(size)
plt.imshow(im);

im = im.convert("L")
plt.imshow(im, cmap='gray');

im = np.array(im)
im

im.max()

print(im.shape)

# load each image, run the initial conversion steps, combine in a list
new_images = []
new_size = (28,28)
for im in images:
  im = Image.open(im).convert("L").resize(new_size)
  plt.imshow(im)
  new_images.append(np.array(im))

len(new_images)

# Look at our letters

print('Look at the letters')
plt.figure(figsize = (12,10))
row, colums = 5, 11
for i in range(55):
    plt.subplot(colums, row, i+1)
    plt.imshow(new_images[i].reshape(28,28),interpolation='nearest', cmap='Greys')
plt.show()

# reshape to add color channel
new_images = np.array(new_images)
new = new_images.reshape(new_images.shape + (1,))

new.shape

x_train_abcde = new

# add labels and one hot encode
train_labels.shape

# encode labels
le = LabelEncoder()
y_train_abcde = le.fit_transform(train_labels)

y_train_abcde.shape

np.unique(y_train_abcde)

"""# Load Test Set"""

# import handwritten letters

test_images = glob.glob("drive/My Drive/test_letters/*.jpg")
test_images

# load each image, run the initial conversion steps, combine in a list
new_test_images = []
new_size = (28,28)
for im in test_images:
  im = Image.open(im).convert("L").resize(new_size)
  plt.imshow(im)
  new_test_images.append(np.array(im))

# Look at our letters

print('Look at the letters')
plt.figure(figsize = (12,10))
row, colums = 1, 5
for i in range(5):
    plt.subplot(colums, row, i+1)
    plt.imshow(new_test_images[i].reshape(28,28),interpolation='nearest', cmap='Greys')
plt.show()

# reshape to add color channel
test = np.array(new_test_images)
new_test = test.reshape(test.shape + (1,))

new_test.shape

x_test_abcde = new_test

test_image_labels = []
for i in test_images:
  test_image_letter = i[28]
  print(test_image_letter)
  test_image_labels.append(test_image_letter)

# encode labels
test_labels = np.array(test_image_labels)
y_test_abcde = le.fit_transform(test_labels)

x_train_abcde.shape, y_train_abcde.shape, x_test_abcde.shape, y_test_abcde.shape

y_train_abcde

y_test_abcde

"""# Transfer Learning"""

# lock the ConvNet layers
layer_trainable = False
for layer in model.layers:
  layer.trainable = layer_trainable
  if layer.name == 'flatten':
    layer_trainable = True

print(f"{'Layer Name':17} {'Is Trainable?'}")
for layer in model.layers:
  print(f"{layer.name:17} {layer.trainable}")

# get the penultimate layer of the model
penult_layer = model.get_layer(name='penult')

# create a new output layer
output_layer = keras.layers.Dense(5, activation='softmax')(penult_layer.output) #change to 5 output values

# create new model with new output layer
new_model = Model(model.input, output_layer)

"""In our new output layer, we want 5 values instead of the 10 that was originally trained on."""

new_model.summary()

new_model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

new_model_hist = new_model.fit(x_train_abcde, y_train_abcde,
                              validation_data=(x_test_abcde, y_test_abcde),
                              batch_size=32,
                              epochs=1000,
                              callbacks=[es])

plot_training_curves(new_model_hist)

"""Based on the accuracy plot, for this run, our model achieves approximately a 60% accuracy. That result is not terribly surprising given the small sample size in addition to the fact that the model was trained on a set of numbers instead of the letters that we were attempting to identify. We suspect our original overfit on our original number data set with close to 100% accuracy and expect this to lead to a poor performing model on a new data set. This is also a case to be cautious when applying transfer learning on a similar but different data set."""

#check model prediction values
pred_new = new_model.predict(x_test_abcde)

pred_new

# argmax along rows to get classification
preds_new = np.argmax(pred_new, axis=1).astype("uint8")

accuracy_score(y_test_abcde, preds_new)

preds_new

y_test_abcde